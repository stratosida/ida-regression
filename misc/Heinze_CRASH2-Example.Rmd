---
title: "Extract from Lecture Notebook"
author: "Georg Heinze"
date: "21 Feb 2020"
output:
  word_document: 
    reference_docx: MB2-Rstudio-template.docx
    toc: true
  html_document:
    df_print: paged
---

# 4. Predictive modeling with the logistic regression model

## Predicting early deaths in patients with traumatic bleeding

CRASH-2 was a large randomised placebo controlled trial among trauma patients with, or at risk of, significant haemorrhage, of the effects of antifibrinolytic treatment on death and transfusion requirement. The study is described at [the original trial website](http://crash2.lshtm.ac.uk/). A public version of the data set is found at a [repository of public data sets](http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets) hosted by the Vanderbilt University's Department of Biostatistics (Prof. Frank Harrell Jr.).

### Download the data

The data set can be downloaded directly as shown below:

```{r}
crash2.url<-url("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/crash2.rda")
load(crash2.url)
dim(crash2)
head(crash2)
```

### Intended statistical analysis

Suppose we are interested in fitting a logistic regression with the purpose of predicting early death in patients with traumatic bleeding. The following variables were deemed important to predict early death:

* Age (`age`, years)
* Sex (`sex`, male or female)
* Systolic blood pressure (`sbp`, mmHg)
* Heart rate (`hr`, 1/min)
* Respiratory rate (`rr`, 1/min)
* Glasgow coma score (`gcs`, points)
* Central capillary refill time (`cc`, seconds)
* Time since injury (`injurytime`, hours)
* Type of injury (`injurytype`, 'blunt', 'penetrating' or 'blunt and penetrating')

The outcome variable, early death (i.e., death within 28 days from injury) must be computed from the time span between date of death and date of randomisation using the following code:

```{r}
# transform ddeath and trandomisation into an interpretable date format and then compute the difference
# interpret NAs as 'not died within study period, at least not within 28 days'
# if patients died after 28 days, treat as alive 
crash2$time2death <- as.numeric(as.Date(crash2$ddeath) - as.Date(crash2$trandomised))

crash2$earlydeath[!is.na(crash2$time2death)] <- (crash2$time2death[!is.na(crash2$time2death)] <= 28) + 0   
# +0 to transform it from TRUE/FALSE to 1/0

crash2$earlydeath[is.na(crash2$time2death)] <- 0    # NA in time2death means alive at day 28

table(crash2$earlydeath)

mean(crash2$earlydeath)

```

The binary outcome variable is now contained in variable `earlydeath`; 1 if died within 28 days, 0 otherwise

We learn from the table above that there were `r table(crash2$earlydeath)[2]` early deaths (`r round(mean(crash2$earlydeath)*100,1)`%).

Other variables that were used in the prediction model reported by [Perel et al, BMJ 2012](https://doi.org/10.1136/bmj.e5166) (economic region, treatment allocation) are not contained in the public use version of the data set and cannot be considered here.


### Split into training and test data set

As our task is prediction modeling, we would like to test our final model using a part of the data which we put aside. That part will be treated just like an independent test cohort and not used for modeling. We will use approximately 25% (approx. 5000 observations) of the data with the most recent date of randomization for testing. This allows for a 'temporal external validation'.

```{r}
n.training <- nrow(crash2) - 5000
day_test <- sort(as.Date(crash2$trandomised))[n.training]
day_test    # all patients randomised after day_test will be included in the test set, those randomised up to that day to the training set

index.training <- as.Date(crash2$trandomised) < day_test
index.test <- as.Date(crash2$trandomised) >= day_test

sum(index.training)
sum(index.test)
```

Vectors `index.training` is TRUE for the first `r sum(index.training)` randomisations, and FALSE afterwards. Therefore, we can extract the training set by subsetting with `index.training`, and the test set by subsetting with `index.test`. We also reduce the size of the data sets by only including the relevant variables for modeling and for description.

```{r}
rel.var <- c("entryid", "trandomised", "age", "sex", "sbp", "hr", "rr", "gcs", "cc", "injurytime","injurytype", "earlydeath")
crash2.train <- crash2[index.training, rel.var]

dim(crash2.train)

crash2.test <- crash2[index.test, rel.var]

dim(crash2.test)
```

### Initial data analysis

```{r}
library(GGally)
crash2.idaplot <- ggpairs(crash2.train, columns=c("age", "sex", "sbp", "hr", "rr", "gcs", "cc", "injurytime", "injurytype"), progress=FALSE)
crash2.idaplot
summary(crash2)
```



```{r}
summary(crash2.train)

# we go deeper into histograms of some skew-looking variables
hist(crash2.train$age)

hist(crash2.train$gcs)

hist(crash2.train$cc)

hist(crash2.train$injurytime)
```

Note that graphs can also be printed into a pdf file, e.g. like in the following code. The pdf output is 'opened' with the `pdf(file="...pdf")` command and closed with `dev.off()`. Don't forget to close it! If you do, all plots produced later will be added to the pdf but that pdf will never be finalized.

```{r}
pdf(file="CRASH2 IDA.pdf")
  crash2.idaplot
  hist(crash2.train$age)
  hist(crash2.train$gcs)
  hist(crash2.train$cc)
  hist(crash2.train$injurytime)
dev.off()
```


Conclusions from initial data analysis:

* there is a single patient with age = 1 year -> omit that person (by setting his/her age to NA)
* `cc` and `injurytime` have quite skew distribution. These variables could be modeled as they to obtain effects per time unit (seconds or hours, respectively). However, with very skew distributions of predictors high values can obtain disproportionally high impact on the result. I usually prefer to transform such variables first. The log-base-2 transformation (`log2()`) is particularly attractive as it still allows to interpret the regression coefficients 'per log2 unit' which essentially means 'per doubling of the original variable'.
* There is relevant correlation between `rr` and `hr`, and between `cc` and `sbp`. (These correlations should be compared with expectations based on clinical expertise.)

As a next step, we perform the exclusion and transformations outlined above and repeat the initial data analysis with the updated data set:

```{r}
# set age of patient with age==1 to NA
crash2.train$age[crash2.train$age == 1] <- NA

# create log2 of cc and injurytime

crash2.train$log2_cc <- log2(crash2.train$cc)
crash2.train$log2_injurytime <- log2(crash2.train$injurytime)

summary(crash2.train$log2_cc)
summary(crash2.train$log2_injurytime)


ggpairs(crash2.train[,c("age", "sex", "sbp", "hr", "rr", "gcs", "log2_cc", "log2_injurytime", "injurytype")], progress=FALSE)


hist(crash2.train$age)
hist(crash2.train$log2_cc)
hist(crash2.train$log2_injurytime)

```
### Data description

Using the  R package `TableOne`, we can create a customary data description table. We will generate columns by injury type.

```{r}
library(tableone)

tabone<-CreateTableOne(vars=c("age", "sex", "sbp", "hr", "rr", "gcs", "cc", "log2_cc", "injurytime", "log2_injurytime"), strata="injurytype", data=crash2.train, test=FALSE)

print(tabone, nonnormal=c("age","sbp","hr","rr","gcs","cc","log2_cc", "injurytime", "log2_injurytime"))
```

We should also evaluate the number and proportions of missing values, per variable and as total number with any missing values.

```{r}
# the apply() function computes per column of a data set (second argument set to 2) or per row of a data set (1)
# the thing we want to compute is two values combined using the c() function: the sum (count) of missing values and the proportion
# is.na(X) is TRUE if a value of X is NA, and FALSE otherwise.
# if we sum over TRUEs and FALSEs, we get the number of TRUE's (they are like 1's)
# if we take the mean over TRUEs and FALSEs, we get the proportion of TRUEs (as they are like 1s)
# the t() function transposes the result (rows<->columns)

out<-t(apply(crash2.train, 2, function(X) c(sum(is.na(X)), mean(is.na(X)))))
# here we assign column names
colnames(out)<- c("Number NAs", "Proportion NAs")

# here we apply per row the any() function which returns TRUE if any of its elements is TRUE
total_na_perobs<-apply(crash2.train, 1, function(X) any(is.na(X)))

# now we put the TOTAL result underneath the variable-wise missing value summary
out<-rbind(out, TOTAL=c(sum(total_na_perobs), mean(total_na_perobs)))

# output rounded to three decimal numbers
round(out,3)
```

By omitting all observations with any missing values in the variables, we loose only about `r round(tail(out,1)[2]*100,1)`% of the data. Given the large initial sample size, this is a feasible way of dealing with the missing data.

### Exercise 4.1

1. (easy) Why is number of NAs in the last line (TOTAL) greater than those of any individual variable?
2. (easy) Why is the number of NAs for `injurytime` the same as for `log2_injurytime`? 


### Modeling strategy

A multivariable logistic regression model will be estimated to predict early death from the independent variables age, sex, systolic blood pressure, heart rate, respiratory rate, Glasgow coma scale, capillary refill time, injury time and injury type.

For continuous variables, we will use natural splines with three degrees of freedom. For `gcs` with its semi-continuous scale, we will just use a polynomial of degree 2. It is unlikely that we observe a U-shaped or even more complex association of `gcs` with early death and therefore a less flexible model with fewer degrees of freedom can be used.

### Multivariable modeling

```{r}
library(splines)
fit.crash2 <- glm(data=crash2.train, formula=earlydeath ~ ns(age, df=3) + sex + ns(sbp, df=3)+ ns(hr, df=3) + ns(rr, df=3)+ poly(gcs, 2, raw=TRUE) + ns(log2_cc, df=3)+ns(log2_injurytime, df=3) + injurytype , family=binomial)

summary(fit.crash2)
```

To visualize the splines fitting, we can create effect plots in which the impact of a variable on the predictions of the model are plotted. The idea is here to compute the predictions from the fitted model given one variable is varied across its full range, and all other variables are held constant at their means (by default, any other values can be chosen). In order to compute effects plots, the package `effects` is needed. If not already available, it can be installed using `install.packages("effects")`.

The following plots depict the role of the continuous predictors in the multivariable prediction model:

```{r}
library(effects)
age.eff <- Effect("age", fit.crash2)
plot(age.eff)

sbp.eff <- Effect("sbp", fit.crash2)
plot(sbp.eff)

hr.eff <- Effect("hr", fit.crash2)
plot(hr.eff)

rr.eff <- Effect("rr", fit.crash2)
plot(rr.eff)

gcs.eff <- Effect("gcs", fit.crash2)
plot(gcs.eff)

log2_cc.eff <- Effect("log2_cc", fit.crash2)
plot(log2_cc.eff)

log2_injurytime.eff <- Effect("log2_injurytime", fit.crash2)
plot(log2_injurytime.eff)
```



### Validation

Validation of a prediction model means: to assess its performance. The result of a validation is not 'yes it's valid' or 'no it's not valid', but rather a couple of performance measures that allow potential users to estimate if the model is useful enough for their purpose or not.

Validation can be performed internally, to see if the model was correctly specified, e.g. using residual plots in the linear model or by computing R-squared values. An external validation evaluates how the model performs if applied in a different population. External validation can be geographical (a population that is in a different geographical area), temporal (if an 'old' model is still performing well enough), or may show if a model can be transferred to a different setting (e.g. from inpatient to outpatient clinic).

Aspects that are usually evaluated when validating a model are its calibration, its discriminative ability, and the explained variation. These aspects are partly related, but may also complement each other; e.g. a model may be highly discriminative but poorly calibrated.

### Discrimination

We make use of the `predict()` function to copmute the predicted probabilites based on the model fit. That function takes care of transforming the continuous variables such that the spline fitting is directly applied to them.

```{r}
pred.train <- predict(fit.crash2, newdata=crash2.train, type="response")

hist(pred.train)
```

The histogram clearly shows that most patients are assigned a low predicted probability, and only few receive a higher predicted probability. 

By comparing the predicted probabilities between patients with event and patients without, we can see how well the model discriminates high-risk from low-risk individuals.

```{r}
boxplot(pred.train~crash2.train$earlydeath, ylab="Predicted probability", xlab="Early death")
```

The impression of that graph that predicted probabilities are higher in those subjects who died within 28 days from injury can be summarized in two numbers:

1. We can compare the mean predicted probabilities between individuals who died and those who survived and compute their mean difference. This measure is known as 'coefficient of discrimination' and can be used as an analogue of R-squared in logistic regression as it ranges between 0 and 1.

2. The predicted probabilities in these two groups of subjects can also be compared nonparametrically using a Mann-Whitney-U-statistic. This number can be interpreted as $Pr(\pi_0 < \pi_1)$, i.e., the probability that a patient who survived was assigned a lower predicted probability than a patient who survived. It is also known as concordance index, c-index or area under the ROC. The c-index has a null value of 0.5 (where the distributions of the probabilities are stochastically equal), and a maximum value of 1. 


Here we 'misuse' a linear model to compute the coefficient of discrimination. The interpretation of the slope, i.e., the coefficient of `earlydeath`, is (as usual) that of an expected difference in the predicted probabilites comparing patients who died to those who survived (= the definition of the coefficient of discrimination).

```{r}
# Computing the coefficient of discrimination
lm(pred.train ~ crash2.train$earlydeath)
```


The coefficient of discrimination in this example is `r coef(lm(pred.train ~ crash2.train$earlydeath))[2]`, which is a good value for a logistic regression model. Ideally, this number should be 1. In the worst case, it will be 0 (meaning that the model is meaningless for the outcome).

The concordance index can be computed using the simple code below:

```{r}
# Simple function to compute the c-index via the Wilcoxon statistic
cindex<-function(x,y) {
  if(is.factor(y)) {
    lev<-levels(y)
    y<-(y==lev[1])*1
  }
  sy0<-sum(y==0)
  sy1<-sum(y==1)
  c1<-wilcox.test(x~y)$statistic/(sy0*sy1)
  if(sy1<sy0) c1<-1-c1
  return(c1)
}

cindex(x=pred.train, y=crash2.train$earlydeath)
```

In this code the Wilcoxon test statistic, which is the sum of ranks of the values (of predicted probabilities) in one group, is divided by the product of the group sizes. This gives the Mann-Whitney statistic which is equal to the concordance index. In the statement `if(sy1<sy0) c1<-1-c1` its value is 'flipped' if it was computed for the smaller group. 

The concordance index is `r round(cindex(x=pred.train, y=crash2.train$earlydeath),3)` and also quite good.

### Calibration

A calibration plot compares the observed outcome rates against the predicted probabilities. To construct such plots, the data are often grouped by the predicted probabilities and then the observed rate in each group is plotted against the mean predicted probability in that group.

We will carry out two calibration evaluations; one in the training set itself (to assess internal calibration), and one in the test set. In the training set we expect good calibration as the model calibrates itself. Still, calibration (e.g. agreement of observed rates and predicted probabilities) can be poor at the edges, i.e. for high-risk individuals.

More interesting is calibration in the test set. Here we compute predicted probabilities from the model, but now using the data on the independent variables of the test set, and compare them to the observed outcomes.


Using the function `calibration()` in the package `caret` we can automate the binning of observations into groups by predicted probabilities. By default, 10 bins are produced. 

```{r}
library(caret)

calib.internal<-calibration(factor(crash2.train$earlydeath, levels=c(1,0))~pred.train, cuts=11)
ggplot(calib.internal)+ylim(c(0,100))+xlim(c(0,100))+ggtitle("Internal calibration")+xlab("Predicted Event Percentage")
```


In the **external (temporal) validation** we apply the model to the data that was not used to fit the model.

First we have to compute `log2_cc` and `log2_injurytime` also in the test set for compatibility of the models. Then we compute the predicted probabilities in the test set applying the model estimated in the training step. This is actually the most important step!

```{r}
crash2.test$log2_cc <- log2(crash2.test$cc)
crash2.test$log2_injurytime <- log2(crash2.test$injurytime)



pred.test <- predict(fit.crash2, newdata=crash2.test, type="response")
```

Now we can proceed by computing coefficient of discrimination and concordance index:

```{r}
# coefficient of discrimination (second coefficient)
lm(pred.test~crash2.test$earlydeath)
```

```{r}
# AUROC or c-index
cindex(x=pred.test, y=crash2.test$earlydeath)
```

Finally we also show the calibration curve for the test set, the ultimate validation of the model.

```{r}
# calibration curve
calib<-calibration(factor(crash2.test$earlydeath, levels=c(1,0))~pred.test, cuts=11)
ggplot(calib)+ylim(c(0,100))+ggtitle("Calibration in test set")+xlab("Predicted Event Percentage")
```

A summary of the calibration curve is obtained by computing the so-called calibration-in-the-large and the calibration slope.

These values are obtained by computing the linear predictors in the test set and then conducting a logistic regression of the actual event status on the linear predictors. The slope (regression coefficient) of that regression is the calibration slope and ideally equal to 1. 

If this is done in the training set, then the intercept will be exactly 0 and the slope exactly 1.

Usually in external validations we observe values for the slope which are slightly smaller than 1. This is called the 'shrinkage' effect: in an independent application of the model, it seems that the effect is smaller than it was anticipated. The reason for this can be overfitting of the effects in the training set (the model adopts too much to the data is no longer fully generalizable), or just because the effects are different in the test population.

In our study we get:

```{r}
# compute linear predictors in the test set
linpred.test <- predict(fit.crash2, newdata=crash2.test, type="link")

# logistic regression on the linear predictors
calib.mod <- glm(data=crash2.test, earlydeath ~ linpred.test, family=binomial)
summary(calib.mod)
```

The calibration slope is indeed slightly smaller than 1, it is `r coef(calib.mod)[2]`. 

To evaluate the calibration-in-the-large, the easiest is to compare the observed event rate with the mean predicted probability in the test set:

```{r}
mean(crash2.test$earlydeath)

# na.rm=TRUE makes sure that NAs are removed before the mean is computed; otherwise the mean would be NA itself
mean(pred.test, na.rm=TRUE)
```

One reason for miscalibration can be a different casemix in the test set. To investigate the comparability of the training and the test set (the 'casemix'), one can compare the linear predictors between the two cohorts:

```{r}
linpred.train <- predict(fit.crash2, crash2.train)

boxplot(linpred.train, linpred.test, ylab="Linear predictors", xlab="Set", axes=F)
axis(1, at=c(1,2), lab=c("Training set", "Test set"))
axis(2)
box()
```




### Exercises 4.2

1. Try to compute a logistic regression of early death on the linear predictors in the training set! What are the intercept and the slope?
2. How do you interpret the coefficient of discrimination in the test set?
3. How do you interpret the c-index in the test set?

